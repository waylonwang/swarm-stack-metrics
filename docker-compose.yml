# Update Time: 2024-02-04 00:10
version: "3.4"

services:
  # Grafana
  grafana:
    image: grafana/grafana:latest
    networks: [network_cluster]
    restart: on-failure
    environment:
      # 直接使用Portainer容器内的环境变量
      PUID: 1000
      PGID: 1000       
      TZ: ${TZ}
    volumes:
      - nfs_grafana_data:/var/lib/grafana
      - nfs_grafana_conf:/usr/share/grafana/conf
    deploy:
      labels:
        - traefik.enable=true
        - traefik.http.routers.grafana.rule=Host(`grafana.${DOMAIN_SWARM}`)
        - traefik.http.routers.grafana.entrypoints=websecure
        - traefik.http.routers.grafana.service=grafana
        - traefik.http.routers.grafana.middlewares=noauth-chain@file
        - traefik.http.services.grafana.loadbalancer.server.port=3000
        - homepage.group=Metrics & Monitor
        - homepage.name=Grafana
        - homepage.icon=grafana.png
        - homepage.href=https://grafana.${DOMAIN_SWARM}:4/
        - homepage.description=Grafana
        - homepage.siteMonitor=http://grafana:3000
        - homepage.weight=10
        - homepage.widget.type=grafana
        - homepage.widget.url=http://grafana:3000
        - homepage.widget.username=${HOMEPAGE_GRAFANA_USERNAME}
        - homepage.widget.password=${HOMEPAGE_GRAFANA_PASSWORD}        
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.type == vm
          - node.labels.metrics == true
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M
          
  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    command: --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/data --web.console.libraries=/usr/share/prometheus/console_libraries --web.enable-lifecycle --web.console.templates=/usr/share/prometheus/consoles
    networks: [network_cluster]
    restart: on-failure
    environment:
      # 直接使用Portainer容器内的环境变量
      PUID: 0
      PGID: 0 
      TZ: ${TZ}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - nfs_prometheus_conf:/etc/prometheus
      - nfs_prometheus_data:/data
    deploy:
      labels:
        - traefik.enable=true
        - traefik.http.routers.prometheus.rule=Host(`prometheus.${DOMAIN_SWARM}`)
        - traefik.http.routers.prometheus.entrypoints=websecure
        - traefik.http.routers.prometheus.service=prometheus
        - traefik.http.routers.prometheus.middlewares=noauth-chain@file
        - traefik.http.services.prometheus.loadbalancer.server.port=9090
        - homepage.group=Metrics & Monitor
        - homepage.name=Prometheus
        - homepage.icon=prometheus.png
        - homepage.href=https://prometheus.${DOMAIN_SWARM}:4/
        - homepage.description=Prometheus
        - homepage.siteMonitor=http://prometheus:9090
        - homepage.weight=11
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.type == vm
          - node.labels.metrics == true
      # resources:
      #   limits:
      #     memory: 512M
      #   reservations:
      #     memory: 64M

  # AlertManager
  alertmanager:
    image: prom/alertmanager:latest
    command:
      - '--config.file=/etc/alertmanager/config.yml'
      - '--storage.path=/alertmanager'
    networks: [network_cluster]
    restart: on-failure
    environment:   
      TZ: ${TZ}
    volumes:
      - nfs_alertmanager:/etc/alertmanager
    deploy:
      placement:
        constraints:
          - node.labels.type == vm
          - node.labels.metrics == true
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M
      labels:
        - traefik.enable=true
        - traefik.http.routers.alertmanager.rule=Host(`alertmanager.${DOMAIN_SWARM}`)
        - traefik.http.routers.alertmanager.entrypoints=websecure
        - traefik.http.routers.alertmanager.service=alertmanager
        - traefik.http.routers.alertmanager.middlewares=noauth-chain@file
        - traefik.http.services.alertmanager.loadbalancer.server.port=9093
        - homepage.group=Metrics & Monitor
        - homepage.name=AlertManager
        - homepage.icon=alertmanager.png
        - homepage.href=https://alertmanager.${DOMAIN_SWARM}:4/
        - homepage.description=Prometheus Alert Manager
        - homepage.siteMonitor=http://alertmanager:9093
        - homepage.weight=12

  # SNMP Exporter
  snmp-exporter:
    image: prom/snmp-exporter:v0.22.0
    command:
      - "--config.file=/etc/snmp_exporter/snmp.yml"
    networks: [network_cluster]
    restart: on-failure
    ports:
      - 9116:9116
    environment:
      TZ: ${TZ}
    volumes:
      - nfs_snmp-exporter:/etc/snmp_exporter
    deploy:
      placement:
        constraints:
          - node.labels.type == vm
          - node.labels.metrics == true
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  # PVE Exporter
  pve-exporter:
    image: prompve/prometheus-pve-exporter:latest
    networks: [network_cluster]
    restart: on-failure
    ports:
      - 9221:9221
    environment:
      PVE_USER: ${PVE_USER}
      PVE_TOKEN_NAME: ${PVE_TOKEN_NAME} 
      PVE_TOKEN_VALUE: ${PVE_TOKEN_VALUE} 
      PVE_VERIFY_SSL: ${PVE_VERIFY_SSL} 
      TZ: ${TZ}
    volumes:
      - nfs_pve-exporter_src:/usr/lib/python3.11/site-packages/pve_exporter
    deploy:
      placement:
        constraints:
          - node.labels.type == vm
          - node.labels.metrics == true
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  # Node Exporter
  # node-exporter:
  #   image: prom/node-exporter:latest
  #   command:
  #     - '--path.procfs=/host/proc'
  #     - '--path.rootfs=/rootfs'
  #     - '--path.sysfs=/host/sys'
  #     - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)'
  #   networks: [network_cluster]
  #   restart: on-failure
  #   ports:
  #     - 9100:9100
  #   environment:   
  #     TZ: ${TZ}
  #   volumes:
  #     - /proc:/etc/pve.yml
  #   deploy:
  #     mode: global
  #     placement:
  #       constraints:
  #         - node.platform.os == linux      
  #     resources:
  #       limits:
  #         memory: 128M
  #       reservations:
  #         memory: 64M

  # Cadvisor
  # cadvisor:
  #   image: docker
  #   volumes: 
  #       - /var/run/docker.sock:/var/run/docker.sock
  #   entrypoint: ["/bin/sh","-c"]
  #   networks: [network_cluster]
  #   deploy:
  #     mode: global
  #     restart_policy:
  #       condition: none
  #   ports:
  #     - 18080:8080
  #   environment:
  #     - TZ=Asia/Shanghai
  #     - PARENT={{.Task.Name}}
  #     - CHILDNAME={{.Service.Name}}_sidecar.{{.Node.ID}}.{{.Task.ID}}
  #   command: 
  #   - |
  #     exec docker run -i --rm --network="container:$${PARENT}" \
  #           --env=TZ=Asia/Shanghai \
  #           --volume=/:/rootfs:ro \
  #           --volume=/var/run:/var/run:ro  \
  #           --volume=/var/run/docker.sock:/var/run/docker.sock:ro \
  #           --volume=/sys:/sys:ro  \
  #           --volume=/var/lib/docker/:/var/lib/docker:ro \
  #           --volume=/dev/disk/:/dev/disk:ro \
  #           --name $${CHILDNAME} \
  #           --privileged \
  #           --device=/dev/kmsg \
  #           registry.${DOMAIN_SWARM}:4/cadvisor:v0.47.3

  # Cadvisor
  # cadvisor:
  #   # image: registry.${DOMAIN_SWARM}:4/cadvisor:v0.47.3
  #   # image: registry.${DOMAIN_SWARM}:4/cadvisor:v0.44.0
  #   image: starlingx/cadvisor:master-debian-stable-latest
  #   networks: [network_cluster]
  #   devices:
  #     - /dev/kmsg
  #   restart: on-failure
  #   ports:
  #     - "8080:8080"    
  #   environment:
  #     TZ: ${TZ}
  #   volumes:
  #     - /:/rootfs:ro
  #     - /var/run:/var/run:rw
  #     - /sys:/sys:ro
  #     - /var/lib/docker/:/var/lib/docker:ro  
  #     - /dev/disk:/dev/disk:ro
  #   deploy:
  #     mode: global
  #     placement:
  #       constraints:
  #         - node.platform.os == linux

  # Sshwifty
  sshwifty:
    image: niruix/sshwifty:latest
    networks: [network_cluster]
    restart: on-failure    
    environment:
      SSHWIFTY_CONFIG: /config/sshwifty.conf.json
    #   - SSHWIFTY_SHAREDKEY=WEB_ACCESS_PASSWORD
    volumes:
      - nfs_sshwifty_conf:/config  
    deploy:
      placement:
        constraints:
          - node.labels.type == vm
          - node.labels.metrics == true
      resources:
        limits:
          cpus: '0.3'
          memory: 600M
      labels:
        - traefik.enable=true
        - traefik.http.routers.ssh.rule=Host(`ssh.${DOMAIN_SWARM}`)
        - traefik.http.routers.ssh.entrypoints=websecure
        - traefik.http.routers.ssh.service=ssh
        - traefik.http.routers.ssh.middlewares=noauth-chain@file
        - traefik.http.services.ssh.loadbalancer.server.port=8182
        - homepage.group=Metrics & Monitor
        - homepage.name=Sshwifty
        - homepage.icon=terminal.png
        - homepage.href=https://ssh.${DOMAIN_SWARM}:4/
        - homepage.description=Sshwifty Web SSH & Telnet Client
        - homepage.siteMonitor=http://ssh:8182
        - homepage.weight=20

networks:
  network_cluster:
    external: true

x-common-keys-volume: &common-keys-volume
  type: nfs
  o: addr=${NFS_SERVER},rw,nfsvers=4

volumes:
  # NFS
  nfs_prometheus_conf:
    driver: local
    driver_opts:
      <<: *common-keys-volume
      device: :${NFS_DEVICE}/metrics/prometheus/conf
  nfs_prometheus_data:
    driver: local
    driver_opts:
      <<: *common-keys-volume
      device: :${NFS_DEVICE}/metrics/prometheus/data
  nfs_alertmanager:
    driver: local
    driver_opts:
      <<: *common-keys-volume
      device: :${NFS_DEVICE}/metrics/alertmanager        
  nfs_grafana_data:
    driver: local
    driver_opts:
      <<: *common-keys-volume
      device: :${NFS_DEVICE}/metrics/grafana/data        
  nfs_grafana_conf:
    driver: local
    driver_opts:
      <<: *common-keys-volume
      device: :${NFS_DEVICE}/metrics/grafana/conf
  nfs_snmp-exporter:
    driver: local
    driver_opts:
      <<: *common-keys-volume
      device: :${NFS_DEVICE}/metrics/snmp-exporter
  nfs_pve-exporter_src:
    driver: local
    driver_opts:
      <<: *common-keys-volume
      device: :${NFS_DEVICE}/metrics/pve-exporter/prometheus-pve-exporter/src/pve_exporter
  nfs_sshwifty_conf:
    driver: local
    driver_opts:
      <<: *common-keys-volume
      device: :${NFS_DEVICE}/metrics/sshwifty/config