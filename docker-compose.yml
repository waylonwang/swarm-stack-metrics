# Update Time: 2023-09-07 14:10
version: "3.4"

services:
  # Grafana
  grafana:
    image: grafana/grafana:latest
    networks: [network_cluster]
    restart: on-failure
    environment:
      # 直接使用Portainer容器内的环境变量
      PUID: 1000
      PGID: 1000       
      TZ: ${TZ}
    volumes:
      - nfs_grafana_data:/var/lib/grafana
      - nfs_grafana_conf:/usr/share/grafana/conf
    deploy:
      labels:
        - traefik.enable=true
        - traefik.http.routers.grafana.rule=Host(`grafana.${DOMAIN_SWARM}`)
        - traefik.http.routers.grafana.entrypoints=websecure
        - traefik.http.routers.grafana.service=grafana
        - traefik.http.routers.grafana.middlewares=noauth-chain@file
        - traefik.http.services.grafana.loadbalancer.server.port=3000
        - homepage.group=Metrics
        - homepage.name=Grafana
        - homepage.icon=grafana.png
        - homepage.href=https://grafana.${DOMAIN_SWARM}:4/
        - homepage.description=Grafana
        - homepage.ping=http://grafana:3000
        - homepage.weight=1
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.type == vm
          - node.labels.metrics == true
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M
          
  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    command: --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.enable-lifecycle --web.console.templates=/usr/share/prometheus/consoles
    networks: [network_cluster]
    restart: on-failure
    environment:
      # 直接使用Portainer容器内的环境变量
      PUID: 0
      PGID: 0 
      TZ: ${TZ}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - nfs_prometheus:/etc/prometheus
    deploy:
      labels:
        - traefik.enable=true
        - traefik.http.routers.prometheus.rule=Host(`prometheus.${DOMAIN_SWARM}`)
        - traefik.http.routers.prometheus.entrypoints=websecure
        - traefik.http.routers.prometheus.service=prometheus
        - traefik.http.routers.prometheus.middlewares=noauth-chain@file
        - traefik.http.services.prometheus.loadbalancer.server.port=9090
        - homepage.group=Metrics
        - homepage.name=Prometheus
        - homepage.icon=prometheus.png
        - homepage.href=https://prometheus.${DOMAIN_SWARM}:4/
        - homepage.description=Prometheus
        - homepage.ping=http://prometheus:9090
        - homepage.weight=2
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.type == vm
          - node.labels.metrics == true
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  # AlertManager
  alertmanager:
    image: prom/alertmanager:latest
    command:
      - '--config.file=/etc/alertmanager/config.yml'
      - '--storage.path=/alertmanager'
    networks: [network_cluster]
    restart: on-failure
    environment:   
      TZ: ${TZ}
    volumes:
      - nfs_alertmanager:/etc/alertmanager
    deploy:
      placement:
        constraints:
          - node.labels.type == vm
          - node.labels.metrics == true
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M
      labels:
        - traefik.enable=true
        - traefik.http.routers.alertmanager.rule=Host(`alertmanager.${DOMAIN_SWARM}`)
        - traefik.http.routers.alertmanager.entrypoints=websecure
        - traefik.http.routers.alertmanager.service=alertmanager
        - traefik.http.routers.alertmanager.middlewares=noauth-chain@file
        - traefik.http.services.alertmanager.loadbalancer.server.port=9093
        - homepage.group=Metrics
        - homepage.name=AlertManager
        - homepage.icon=alertmanager.png
        - homepage.href=https://alertmanager.${DOMAIN_SWARM}:4/
        - homepage.description=Prometheus Alert Manager
        - homepage.ping=http://alertmanager:9093
        - homepage.weight=3

  # PVE Exporter
  pve-exporter:
    image: prompve/prometheus-pve-exporter:latest
    networks: [network_cluster]
    restart: on-failure
    ports:
      - 9221:9221
    environment:   
      TZ: ${TZ}
    volumes:
      - nfs_pve-exporter_conf:/etc
    deploy:
      placement:
        constraints:
          - node.labels.type == vm
          - node.labels.metrics == true
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  # Node Exporter
  node-exporter:
    image: prom/node-exporter:latest
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)'
    networks: [network_cluster]
    restart: on-failure
    ports:
      - 9100:9100
    environment:   
      TZ: ${TZ}
    volumes:
      - /proc:/etc/pve.yml
    deploy:
      mode: global
      placement:
        constraints:
          - node.platform.os == linux      
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  # Cadvisor
  cadvisor:
    image: docker
    volumes: 
        - /var/run/docker.sock:/var/run/docker.sock
    entrypoint: ["/bin/sh","-c"]
    networks: [network_cluster]
    deploy:
      mode: global
      restart_policy:
        condition: none
    ports:
      - 18080:8080
    environment:
      - TZ=Asia/Shanghai
      - PARENT={{.Task.Name}}
      - CHILDNAME={{.Service.Name}}_sidecar.{{.Node.ID}}.{{.Task.ID}}
    command: 
    - |
      exec docker run -i --rm --network="container:$${PARENT}" \
            --env=TZ=Asia/Shanghai \
            --volume=/:/rootfs:ro \
            --volume=/var/run:/var/run:ro  \
            --volume=/var/run/docker.sock:/var/run/docker.sock:ro \
            --volume=/sys:/sys:ro  \
            --volume=/var/lib/docker/:/var/lib/docker:ro \
            --volume=/dev/disk/:/dev/disk:ro \
            --name $${CHILDNAME} \
            --privileged \
            --device=/dev/kmsg \
            registry.cloudvalley.name:4/cadvisor:v0.47.3

  # Cadvisor
  # cadvisor:
  #   # image: registry.cloudvalley.name:4/cadvisor:v0.47.3
  #   # image: registry.cloudvalley.name:4/cadvisor:v0.44.0
  #   image: starlingx/cadvisor:master-debian-stable-latest
  #   networks: [network_cluster]
  #   devices:
  #     - /dev/kmsg
  #   restart: on-failure
  #   ports:
  #     - "8080:8080"    
  #   environment:
  #     TZ: ${TZ}
  #   volumes:
  #     - /:/rootfs:ro
  #     - /var/run:/var/run:rw
  #     - /sys:/sys:ro
  #     - /var/lib/docker/:/var/lib/docker:ro  
  #     - /dev/disk:/dev/disk:ro
  #   deploy:
  #     mode: global
  #     placement:
  #       constraints:
  #         - node.platform.os == linux

networks:
  network_cluster:
    external: true

x-common-keys-volume: &common-keys-volume
  type: nfs
  o: addr=${NFS_SERVER},rw,nfsvers=4

volumes:
  # NFS
  nfs_prometheus:
    driver: local
    driver_opts:
      <<: *common-keys-volume
      device: :${NFS_DEVICE}/metrics/prometheus  
  nfs_alertmanager:
    driver: local
    driver_opts:
      <<: *common-keys-volume
      device: :${NFS_DEVICE}/metrics/alertmanager        
  nfs_grafana_data:
    driver: local
    driver_opts:
      <<: *common-keys-volume
      device: :${NFS_DEVICE}/metrics/grafana/data        
  nfs_grafana_conf:
    driver: local
    driver_opts:
      <<: *common-keys-volume
      device: :${NFS_DEVICE}/metrics/grafana/conf  
  nfs_pve-exporter_conf:
    driver: local
    driver_opts:
      <<: *common-keys-volume
      device: :${NFS_DEVICE}/metrics/pve-exporter             