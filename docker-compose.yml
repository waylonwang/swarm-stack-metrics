# Update Time: 2023-09-07 14:10
version: "3.4"

services:
  # Grafana
  grafana:
    image: grafana/grafana:latest
    networks: [network_cluster]
    restart: on-failure
    environment:
      # 直接使用Portainer容器内的环境变量
      PUID: 1000
      PGID: 1000       
      TZ: ${TZ}
    volumes:
      - nfs_grafana_data:/var/lib/grafana
      - nfs_grafana_conf:/usr/share/grafana/conf
    deploy:
      labels:
        - "traefik.enable=true"
        - "traefik.http.routers.grafana.rule=Host(`grafana.${DOMAIN_SWARM}`)"
        - "traefik.http.routers.grafana.entrypoints=websecure"
        - "traefik.http.routers.grafana.service=grafana"
        - "traefik.http.routers.grafana.middlewares=noauth-chain@file"
        - "traefik.http.services.grafana.loadbalancer.server.port=3000"
        - homepage.group=Metrics
        - homepage.name=Grafana
        - homepage.icon=grafana.png
        - homepage.href=https://grafana.${DOMAIN_SWARM}:4/
        - homepage.description=Grafana
        - homepage.ping=http://grafana:3000
        - homepage.weight=1
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.type == vm
          - node.labels.metrics == true
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M     
          
  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    command: --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.enable-lifecycle --web.console.templates=/usr/share/prometheus/consoles
    networks: [network_cluster]
    restart: on-failure
    environment:
      # 直接使用Portainer容器内的环境变量
      PUID: 1000
      PGID: 1000       
      TZ: ${TZ}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - nfs_prometheus:/etc/prometheus
    deploy:
      labels:
        - traefik.enable=true
        - traefik.http.routers.prometheus.rule=Host(`prometheus.${DOMAIN_SWARM}`)
        - traefik.http.routers.prometheus.entrypoints=websecure
        - traefik.http.routers.prometheus.service=prometheus
        - traefik.http.routers.prometheus.middlewares=noauth-chain@file
        - traefik.http.services.prometheus.loadbalancer.server.port=9090
        - homepage.group=Metrics
        - homepage.name=Prometheus
        - homepage.icon=prometheus.png
        - homepage.href=https://prometheus.${DOMAIN_SWARM}:4/
        - homepage.description=Prometheus
        - homepage.ping=http://prometheus:9090
        - homepage.weight=2
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.type == vm
          - node.labels.metrics == true
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M          

  # AlertManager
  alertmanager:
    image: prom/alertmanager:latest
    command:
      - '--config.file=/etc/alertmanager/config.yml'
      - '--storage.path=/alertmanager'
    networks: [network_cluster]
    restart: on-failure
    environment:   
      TZ: ${TZ}
    volumes:
      - nfs_alertmanager:/etc/alertmanager
    deploy:
      placement:
        constraints:
          - node.labels.type == vm
          - node.labels.metrics == true
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M
      labels:
        - traefik.enable=true
        - traefik.http.routers.alertmanager.rule=Host(`alertmanager.${DOMAIN_SWARM}`)
        - traefik.http.routers.alertmanager.entrypoints=websecure
        - traefik.http.routers.alertmanager.service=alertmanager
        - traefik.http.routers.alertmanager.middlewares=noauth-chain@file
        - traefik.http.services.alertmanager.loadbalancer.server.port=9093
        - homepage.group=Metrics
        - homepage.name=AlertManager
        - homepage.icon=alertmanager.png
        - homepage.href=https://alertmanager.${DOMAIN_SWARM}:4/
        - homepage.description=Prometheus Alert Manager
        - homepage.ping=http://alertmanager:9093
        - homepage.weight=3

  node-exporter:
    image: prom/node-exporter:latest
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)'
    networks: [network_cluster]
    restart: on-failure
    ports:
      - "9100:9100"    
    environment:   
      TZ: ${TZ}
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
      - /etc/hostname:/etc/nodename
    deploy:
      mode: global
      placement:
        constraints:
          - node.platform.os == linux      
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  # cadvisor:
  #   image: docker
  #   volumes: 
  #       - /var/run/docker.sock:/var/run/docker.sock
  #   entrypoint: ["/bin/sh","-c"]
  #   networks: [network_cluster]
  #   deploy:
  #     mode: global
  #     restart_policy:
  #       condition: none
  #   environment:
  #     - TZ=Asia/Shanghai
  #     - PARENT={{.Task.Name}}
  #     - CHILDNAME={{.Service.Name}}_sidecar.{{.Node.ID}}.{{.Task.ID}}
  #   command: 
  #   - |
  #     exec docker run -i --rm --network="container:$${PARENT}" \
  #           --env=TZ=Asia/Shanghai \
  #           --volume=/:/rootfs:ro \
  #           --volume=/var/run:/var/run:ro  \
  #           --volume=/var/run/docker.sock:/var/run/docker.sock:ro \
  #           --volume=/sys:/sys:ro  \
  #           --volume=/var/lib/docker/:/var/lib/docker:ro \
  #           --volume=/dev/disk/:/dev/disk:ro \
  #           --name $${CHILDNAME} \
  #           --privileged \
  #           --device=/dev/kmsg \
  #           cjie001/cadvisor:0.37.5
  # Cadvisor
  cadvisor:
    image: starlingx/cadvisor
    networks: [network_cluster]
    devices:
      - /dev/kmsg:/dev/kmsg
    restart: on-failure    
    ports:
      - "8888:8080"    
    environment:
      TZ: ${TZ}
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro  
    deploy:
      mode: global
      placement:
        constraints:
          - node.platform.os == linux
  # cadvisor:
  #   image: gcr.io/cadvisor/cadvisor:v0.47.0
  #   command: -logtostderr -docker_only
  #   deploy:
  #     mode: global
  #     resources:
  #       limits:
  #         memory: 128M
  #       reservations:
  #         memory: 64M
  #   volumes:
  #     - type: bind
  #       source: /
  #       target: /rootfs
  #       read_only: true
  #     - type: bind
  #       source: /var/run
  #       target: /var/run
  #       read_only: true
  #     - type: bind
  #       source: /sys
  #       target: /sys
  #       read_only: true
  #     - type: bind
  #       source: /var/lib/docker
  #       target: /var/lib/docker
  #       read_only: true
  #     - type: bind
  #       source: /dev/disk
  #       target: /dev/disk
  #       read_only: true                        
  #   networks: [network_cluster]

networks:
  network_cluster:
    external: true

x-common-keys-volume: &common-keys-volume
  type: nfs
  o: addr=${NFS_SERVER},rw,nfsvers=4

volumes:
  # NFS
  nfs_prometheus:
    driver: local
    driver_opts:
      <<: *common-keys-volume
      device: :${NFS_DEVICE}/metrics/prometheus  
  nfs_alertmanager:
    driver: local
    driver_opts:
      <<: *common-keys-volume
      device: :${NFS_DEVICE}/metrics/alertmanager        
  nfs_grafana_data:
    driver: local
    driver_opts:
      <<: *common-keys-volume
      device: :${NFS_DEVICE}/metrics/grafana/data        
  nfs_grafana_conf:
    driver: local
    driver_opts:
      <<: *common-keys-volume
      device: :${NFS_DEVICE}/metrics/grafana/conf         